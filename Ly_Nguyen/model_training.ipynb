{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.1694807625032735e-26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from dataloader import CustomDataloader\n",
    "from utils import CustomDataset\n",
    "import tqdm\n",
    "\n",
    "features = ['age_st', 'haircuts_st', 'gender_gender_female', 'gender_gender_male',\n",
    "            'has_tiktok_has_tiktok_no', 'has_tiktok_has_tiktok_yes',\n",
    "            'remembers_disco_remembers_disco_no', 'remembers_disco_remembers_disco_yes',\n",
    "            'uses_skincare_uses_skincare_no', 'uses_skincare_uses_skincare_yes']\n",
    "target = ['age']\n",
    "\n",
    "train_df = pd.read_csv('data/csv_training.csv')\n",
    "\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(train_df[features], train_df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# dataframes -> numpy arrays -> tensors\n",
    "x_train_1, x_test_1 = x_train_1.to_numpy(), x_test_1.to_numpy()\n",
    "y_train_1, y_test_1 = y_train_1.to_numpy(), y_test_1.to_numpy()\n",
    "\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(x_train_1, y_train_1)\n",
    "\n",
    "model_1_predict = model_1.predict(x_test_1)\n",
    "\n",
    "mse = mean_squared_error(y_test_1, model_1_predict)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, output_dim: int):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        assert output_dim > 0, \"Output dimension must be a positive integer\"\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 1,\n",
    "            out_channels = 16,\n",
    "            kernel_size = (5, 5), \n",
    "            stride = (1, 1),\n",
    "            padding = (0, 0)\n",
    "        )\n",
    "        self.maxpool1 = nn.MaxPool2d(\n",
    "            kernel_size = (3,3),\n",
    "            stride = (2,2),\n",
    "            padding = (0,0)\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 16, \n",
    "            out_channels = 64, \n",
    "            kernel_size = (3, 3), \n",
    "            stride = (2, 2), \n",
    "            padding = (0, 0)\n",
    "        )\n",
    "        self.maxpool2 = nn.MaxPool2d(\n",
    "            kernel_size = (5,5),\n",
    "            stride = (2,2),\n",
    "            padding = (0,0)\n",
    "        )\n",
    "        self.linear1 = nn.Linear(\n",
    "            in_features=64,\n",
    "            out_features=output_dim\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        # reshape for linear layer\n",
    "        # note that the output of maxpool 2 is (*,64,1,1) so we just need to take the first column and row. \n",
    "        # If the output size is not 1,1, we have to flatten x before going into linear using torch.flatten\n",
    "        x = x[:,:,0,0] \n",
    "        x = self.linear1(x)     \n",
    "        x = torch.sigmoid(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data and labels (x and y) ready\n",
    "# Replace this with your actual data\n",
    "df = pd.read_csv('data/UTKFaceAugmented.csv')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define transformations for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 16  # Set your desired batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor([ 8617,  6879, 13406,  ..., 10739,  3570, 16448])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\.venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\.venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:155\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor([ 8617,  6879, 13406,  ..., 10739,  3570, 16448])",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\Ly_Nguyen\\model_training.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ly/Documents/ECEGR4750/Ly_Nguyen/model_training.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_dataloader\u001b[39m.\u001b[39mnum_batches_per_epoch):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ly/Documents/ECEGR4750/Ly_Nguyen/model_training.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# training data forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ly/Documents/ECEGR4750/Ly_Nguyen/model_training.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ly/Documents/ECEGR4750/Ly_Nguyen/model_training.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     train_batch \u001b[39m=\u001b[39m train_dataloader\u001b[39m.\u001b[39;49mfetch_batch()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ly/Documents/ECEGR4750/Ly_Nguyen/model_training.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     yhat \u001b[39m=\u001b[39m model(train_batch[\u001b[39m'\u001b[39m\u001b[39mx_batch\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ly/Documents/ECEGR4750/Ly_Nguyen/model_training.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(loss_fn(yhat, train_batch[\u001b[39m'\u001b[39m\u001b[39my_batch\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\Ly_Nguyen\\dataloader.py:70\u001b[0m, in \u001b[0;36mCustomDataloader.fetch_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch_batch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     68\u001b[0m     \u001b[39m# if the iter hasn't been generated yet\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_iter()\n\u001b[0;32m     72\u001b[0m     \u001b[39m# fetch the next batch\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter)\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\Ly_Nguyen\\dataloader.py:37\u001b[0m, in \u001b[0;36mCustomDataloader.generate_iter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mThis function converts the dataset into a sequence of batches, and wraps it in\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39man iterable that can be called to efficiently fetch one batch at a time\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandomize:\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandomize_dataset()\n\u001b[0;32m     39\u001b[0m \u001b[39m# split dataset into sequence of batches \u001b[39;00m\n\u001b[0;32m     40\u001b[0m batches \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\Ly_Nguyen\\dataloader.py:27\u001b[0m, in \u001b[0;36mCustomDataloader.randomize_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mThis function randomizes the dataset, while maintaining the relationship between \u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mx and y tensors\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx[indices]\n\u001b[0;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[indices]\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\.venv\\lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\.venv\\lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Ly\\Documents\\ECEGR4750\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor([ 8617,  6879, 13406,  ..., 10739,  3570, 16448])"
     ]
    }
   ],
   "source": [
    "train_dataloader = CustomDataloader(x = x_train, y = y_train, batch_size=16, randomize=True)\n",
    "val_dataloader = CustomDataloader(x = x_val, y = y_val, batch_size=16, randomize=False)\n",
    "\n",
    "model = CNNClassifier(32)\n",
    "\n",
    "# instantiate your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# log your losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# define how many epochs to train on\n",
    "epochs = 25\n",
    "\n",
    "# define your loss function for multiclass classification task\n",
    "# BCE does binary cross entropy automatically for each class\n",
    "loss_fn = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "for _ in tqdm.tqdm(range(epochs)):\n",
    "    losses = []\n",
    "    for _ in range(train_dataloader.num_batches_per_epoch):\n",
    "        # training data forward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_batch = train_dataloader.fetch_batch()\n",
    "        yhat = model(train_batch['x_batch'])\n",
    "        train_loss = torch.mean(loss_fn(yhat, train_batch['y_batch']))\n",
    "\n",
    "        # training data backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(train_loss.detach().numpy())\n",
    "\n",
    "    # personally, I like to visualize the loss per every iteration, rather than every epoch. I find it more useful to diagnose issues\n",
    "    train_losses.extend(losses)\n",
    "    \n",
    "    losses = []\n",
    "    for _ in range(val_dataloader.num_batches_per_epoch):\n",
    "        # validation data forward pass only\n",
    "        val_batch = val_dataloader.fetch_batch()\n",
    "        yhat = model(val_batch['x_batch'])\n",
    "        val_loss = torch.mean(loss_fn(yhat, val_batch['y_batch']),axis=0)\n",
    "        losses.append(val_loss.detach().numpy())\n",
    "    # epoch-level logging for validation though usually makes the most sense\n",
    "    val_losses.append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
